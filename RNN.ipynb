{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\sumona\\anaconda3\\lib\\site-packages (3.3.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib) (1.20.1)\n",
      "Requirement already satisfied: six in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moabb in c:\\users\\sumona\\anaconda3\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.19.0 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from moabb) (1.20.1)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.0.0 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from moabb) (3.3.4)\n",
      "Requirement already satisfied: mne>=0.19 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from moabb) (0.23.4)\n",
      "Requirement already satisfied: scipy<2.0,>=1.5 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from moabb) (1.6.2)\n",
      "Requirement already satisfied: PyYAML<6.0.0,>=5.0.0 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from moabb) (5.4.1)\n",
      "Requirement already satisfied: h5py<4.0.0,>=3.0.0 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from moabb) (3.4.0)\n",
      "Requirement already satisfied: coverage<6.0.0,>=5.5.0 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from moabb) (5.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.15.1 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from moabb) (2.25.1)\n",
      "Requirement already satisfied: seaborn>=0.9.0 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from moabb) (0.11.1)\n",
      "Requirement already satisfied: scikit-learn<0.24,>=0.23 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from moabb) (0.23.2)\n",
      "Requirement already satisfied: pooch<2.0.0,>=1.3.0 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from moabb) (1.5.2)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from moabb) (1.2.4)\n",
      "Requirement already satisfied: pyriemann>=0.2.6 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from moabb) (0.2.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->moabb) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->moabb) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->moabb) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->moabb) (8.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->moabb) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib<4.0.0,>=3.0.0->moabb) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from pandas<2.0.0,>=1.0.0->moabb) (2021.1)\n",
      "Requirement already satisfied: appdirs in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from pooch<2.0.0,>=1.3.0->moabb) (1.4.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from pooch<2.0.0,>=1.3.0->moabb) (20.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from pyriemann>=0.2.6->moabb) (1.0.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.15.1->moabb) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.15.1->moabb) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.15.1->moabb) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.15.1->moabb) (2.10)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from scikit-learn<0.24,>=0.23->moabb) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install moabb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: braindecode in c:\\users\\sumona\\anaconda3\\lib\\site-packages (0.5.1)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: skorch in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from braindecode) (0.10.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from braindecode) (3.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from braindecode) (3.3.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from braindecode) (1.20.1)\n",
      "Requirement already satisfied: mne in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from braindecode) (0.23.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from braindecode) (1.6.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from braindecode) (1.2.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib->braindecode) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib->braindecode) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib->braindecode) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib->braindecode) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from matplotlib->braindecode) (8.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->braindecode) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from pandas->braindecode) (2021.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from skorch->braindecode) (0.8.9)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from skorch->braindecode) (0.23.2)\n",
      "Requirement already satisfied: tqdm>=4.14.0 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from skorch->braindecode) (4.59.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->skorch->braindecode) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->skorch->braindecode) (1.0.1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install braindecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\sumona\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\sumona\\anaconda3\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from torchvision) (1.20.1)\n",
      "Requirement already satisfied: torch==1.9.1 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from torchvision) (1.9.1)\n",
      "Requirement already satisfied: pillow>=5.3.0 in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\sumona\\anaconda3\\lib\\site-packages (from torch==1.9.1->torchvision) (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-623fa5b0b93e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.util import set_random_seeds, np_to_var, var_to_np\n",
    "import matplotlib.pyplot as plt\n",
    "from moabb.datasets import BNCI2014001, Cho2017, PhysionetMI\n",
    "from moabb.paradigms import MotorImagery\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import mne\n",
    "\n",
    "from util.Motor_Net9 import DpEEG_net\n",
    "from util.utilfunc import get_balanced_batches\n",
    "from util.preproc import plot_confusion_matrix\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "print('gpu: ', cuda)\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "rng = RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " mne.set_config('MNE_DATA', 'D:\\EEG_Motor_Imagery\\Dataset')\n",
    " mne.set_config('MNE_DATASETS_BNCI_PATH', 'D:\\EEG_Motor_Imagery\\Dataset')\n",
    " mne.set_config('MNE_DATASETS_EEGBCI_PATH', 'D:\\EEG_Motor_Imagery\\Dataset')\n",
    " mne.set_config('MNE_DATASETS_GIGADB_PATH', 'D:\\EEG_Motor_Imagery\\Dataset')\n",
    " print(mne.get_config())\n",
    "# C:\\Users\\User\\mne-data\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "ds_src1 = Cho2017()\n",
    "ds_src2 = PhysionetMI()\n",
    "ds_tgt = BNCI2014001()\n",
    "\n",
    "ds_src1_sub = [ i for i in range(1 , 32)] + [ i for i in range(33 , 46)] + [ i for i in range(47 , 49)] + [ i for i in range(50 , 53)]# last = 52\n",
    "\n",
    "ds_src2_sub = [ i for i in range(1 , 110)] # last = 109\n",
    "\n",
    "ds_tgt_sub = [ i for i in range(1 , 10)] # last = 9\n",
    "\n",
    "\n",
    "fmin, fmax = 0, 60\n",
    "raw = ds_tgt.get_data(subjects=[1])[1]['session_T']['run_1']\n",
    "tgt_channels = raw.pick_types(eeg=True).ch_names\n",
    "sfreq = 250.\n",
    "prgm_2classes = MotorImagery(n_classes=2, channels=tgt_channels, resample=sfreq, fmin=fmin, fmax=fmax)\n",
    "prgm_4classes = MotorImagery(n_classes=4, channels=tgt_channels, resample=sfreq, fmin=fmin, fmax=fmax)\n",
    "\n",
    "X_src1, label_src1, m_src1 = prgm_2classes.get_data(dataset=ds_src1, subjects=ds_src1_sub)\n",
    "print('Cho2017 done')\n",
    "end = time.time()\n",
    "print('time is {} min'.format(int((end-start)/60)))\n",
    "\n",
    "X_src2, label_src2, m_src2 = prgm_4classes.get_data(dataset=ds_src2, subjects=ds_src2_sub)\n",
    "print('PhysionetMI done')\n",
    "end = time.time()\n",
    "print('time is {} min'.format(int((end-start)/60)))\n",
    "X_tgt, label_tgt, m_tgt = prgm_4classes.get_data(dataset=ds_tgt, subjects=ds_tgt_sub)\n",
    "print('BNCI2014001 done')\n",
    "end = time.time()\n",
    "print('time is {} min'.format(int((end-start)/60)))\n",
    "\n",
    "print(\"First source dataset has {} trials with {} electrodes and {} time samples\".format(*X_src1.shape))\n",
    "print(\"Second source dataset has {} trials with {} electrodes and {} time samples\".format(*X_src2.shape))\n",
    "print(\"Target dataset has {} trials with {} electrodes and {} time samples\".format(*X_tgt.shape))\n",
    "\n",
    "print (\"\\nSource dataset 1 include labels: {}\".format(np.unique(label_src1)))\n",
    "print (\"Source dataset 2 include labels: {}\".format(np.unique(label_src2)))\n",
    "print (\"Target dataset 1 include labels: {}\".format(np.unique(label_tgt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel(l):\n",
    "    if l == 'left_hand': return 0\n",
    "    elif l == 'right_hand': return 1\n",
    "    else: return 2\n",
    "\n",
    "\n",
    "y_src1 = np.array([relabel(l) for l in label_src1])\n",
    "y_src2 = np.array([relabel(l) for l in label_src2])\n",
    "y_tgt = np.array([relabel(l) for l in label_tgt])\n",
    "\n",
    "print(\"Only right-/left-hand labels are used and first source dataset does not have other labels:\")\n",
    "print(np.unique(y_src1), np.unique(y_src2), np.unique(y_tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_tgt.shape)\n",
    "valid_size = int(X_tgt.shape[0]*.2)\n",
    "test_size = int(X_tgt.shape[0]*.2)\n",
    "# print(test_size)\n",
    "train_size = X_tgt.shape[0] - (test_size + valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = min(X_src1.shape[2], X_src2.shape[2], X_tgt.shape[2])\n",
    "\n",
    "X_train = np.concatenate((X_src1[:, :, :window_size], X_src2[:, :, :window_size], X_tgt[:train_size, :, :window_size]))\n",
    "y_train = np.concatenate((y_src1, y_src2, y_tgt[:train_size]))\n",
    "\n",
    "X_val = X_tgt[train_size:(train_size+valid_size), :, :window_size]\n",
    "y_val = y_tgt[train_size:(train_size+valid_size)]\n",
    "\n",
    "X_test = X_tgt[(train_size+valid_size):, :, :window_size]\n",
    "y_test = y_tgt[(train_size+valid_size):]\n",
    "\n",
    "print(\"Train:  there are {} trials with {} electrodes and {} time samples\".format(*X_train.shape))\n",
    "print(\"\\nValidation: there are {} trials with {} electrodes and {} time samples\".format(*X_val.shape))\n",
    "print(\"\\nTest: there are {} trials with {} electrodes and {} time samples\".format(*X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainObject(object):\n",
    "    def __init__(self, X, y):\n",
    "        assert len(X) == len(y)\n",
    "        # Normalised, you could choose other normalisation strategy\n",
    "        mean = np.mean(X,axis=1,keepdims=True)\n",
    "        # here normalise across channels as an example, unlike the in the sleep kit\n",
    "        std = np.std(X, axis=1, keepdims=True)\n",
    "        X = (X - mean) / std\n",
    "        # we scale it to 1000 as a better training scale of the shallow CNN\n",
    "        # according to the orignal work of the paper referenced above\n",
    "        self.X = X.astype(np.float32) * 1e3\n",
    "        self.y = y.astype(np.int64)\n",
    "\n",
    "train_set = TrainObject(X_train, y=y_train)\n",
    "valid_set = TrainObject(X_val, y=y_val)\n",
    "test_set = TrainObject(X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "# Hide the Configuration and Warnings\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from Models.DatasetAPI.DataLoader import DatasetLoader\n",
    "from Models.Network.Thin_ResNet import Thin_ResNet\n",
    "from Models.Loss_Function.Loss import loss\n",
    "from Models.Evaluation_Metrics.Metrics import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from Models.Initialize_Variables.Initialize import *\n",
    "\n",
    "\n",
    "def Thin_ResNet(Input, keep_prob):\n",
    "    '''\n",
    "    Args:\n",
    "        Input: The reshaped input EEG signals\n",
    "        keep_prob: The Keep probability of Dropout\n",
    "    Returns:\n",
    "        prediction: Final prediction of Thin ResNet Model\n",
    "    '''\n",
    "\n",
    "    # Input reshaped EEG signals: shape 4096 --> 64 X 64\n",
    "    x_Reshape = tf.reshape(tensor=Input, shape=[-1, 64, 64, 1])\n",
    "\n",
    "    # First Residual Block\n",
    "    # First Convolutional Layer\n",
    "    W_conv1 = weight_variable([1, 1, 1, 48])\n",
    "    b_conv1 = bias_variable([48])\n",
    "    h_conv1 = tf.nn.conv2d(x_Reshape, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1\n",
    "    h_conv1_BN = tf.layers.batch_normalization(h_conv1, training=True)\n",
    "    h_conv1_Acti = tf.nn.leaky_relu(h_conv1_BN)\n",
    "    h_conv1_drop = tf.nn.dropout(h_conv1_Acti, keep_prob, noise_shape=[tf.shape(h_conv1_Acti)[0], 1, 1, tf.shape(h_conv1_Acti)[3]])\n",
    "\n",
    "    # Second Convolutional Layer\n",
    "    W_conv2 = weight_variable([3, 3, 48, 48])\n",
    "    b_conv2 = bias_variable([48])\n",
    "    h_conv2 = tf.nn.conv2d(h_conv1_drop, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2\n",
    "    h_conv2_BN = tf.layers.batch_normalization(h_conv2, training=True)\n",
    "    h_conv2_Acti = tf.nn.leaky_relu(h_conv2_BN)\n",
    "    h_conv2_drop = tf.nn.dropout(h_conv2_Acti, keep_prob, noise_shape=[tf.shape(h_conv2_Acti)[0], 1, 1, tf.shape(h_conv2_Acti)[3]])\n",
    "\n",
    "    # Third Convolutional Layer\n",
    "    W_conv3 = weight_variable([1, 1, 48, 96])\n",
    "    b_conv3 = bias_variable([96])\n",
    "    h_conv3 = tf.nn.conv2d(h_conv2_drop, W_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3\n",
    "    h_conv3_BN = tf.layers.batch_normalization(h_conv3, training=True)\n",
    "    h_conv3_res = tf.concat([h_conv3_BN, x_Reshape], axis=3)  # 97 feature maps now == 96 + 1\n",
    "    h_conv3_Acti = tf.nn.leaky_relu(h_conv3_res)\n",
    "    h_conv3_drop = tf.nn.dropout(h_conv3_Acti, keep_prob, noise_shape=[tf.shape(h_conv3_Acti)[0], 1, 1, tf.shape(h_conv3_Acti)[3]])\n",
    "\n",
    "    # First Max Pooling Layer: shape 64 X 64 --> 32 X 32\n",
    "    h_pool1 = tf.nn.max_pool(h_conv3_drop, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Second Residual Block\n",
    "    # Fourth Convolutional Layer\n",
    "    W_conv4 = weight_variable([1, 1, 97, 96])\n",
    "    b_conv4 = bias_variable([96])\n",
    "    h_conv4 = tf.nn.conv2d(h_pool1, W_conv4, strides=[1, 1, 1, 1], padding='SAME') + b_conv4\n",
    "    h_conv4_BN = tf.layers.batch_normalization(h_conv4, training=True)\n",
    "    h_conv4_Acti = tf.nn.leaky_relu(h_conv4_BN)\n",
    "    h_conv4_drop = tf.nn.dropout(h_conv4_Acti, keep_prob, noise_shape=[tf.shape(h_conv4_Acti)[0], 1, 1, tf.shape(h_conv4_Acti)[3]])\n",
    "\n",
    "    # Fifth Convolutional Layer\n",
    "    W_conv5 = weight_variable([3, 3, 96, 96])\n",
    "    b_conv5 = bias_variable([96])\n",
    "    h_conv5 = tf.nn.conv2d(h_conv4_drop, W_conv5, strides=[1, 1, 1, 1], padding='SAME') + b_conv5\n",
    "    h_conv5_BN = tf.layers.batch_normalization(h_conv5, training=True)\n",
    "    h_conv5_Acti = tf.nn.leaky_relu(h_conv5_BN)\n",
    "    h_conv5_drop = tf.nn.dropout(h_conv5_Acti, keep_prob, noise_shape=[tf.shape(h_conv5_Acti)[0], 1, 1, tf.shape(h_conv5_Acti)[3]])\n",
    "\n",
    "    # Sixth Convolutional Layer\n",
    "    W_conv6 = weight_variable([1, 1, 96, 128])\n",
    "    b_conv6 = bias_variable([128])\n",
    "    h_conv6 = tf.nn.conv2d(h_conv5_drop, W_conv6, strides=[1, 1, 1, 1], padding='SAME') + b_conv6\n",
    "    h_conv6_BN = tf.layers.batch_normalization(h_conv6, training=True)\n",
    "    h_conv6_res = tf.concat([h_conv6_BN, h_pool1], axis=3)  # 225 feature maps now == 97 + 128\n",
    "    h_conv6_Acti = tf.nn.leaky_relu(h_conv6_res)\n",
    "    h_conv6_drop = tf.nn.dropout(h_conv6_Acti, keep_prob, noise_shape=[tf.shape(h_conv6_Acti)[0], 1, 1, tf.shape(h_conv6_Acti)[3]])\n",
    "\n",
    "    # Second Max Pooling Layer: shape 32 X 32 --> 16 X 16\n",
    "    h_pool2 = tf.nn.max_pool(h_conv6_drop, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Third Residual Block\n",
    "    # Seventh Convolutional Layer\n",
    "    W_conv7 = weight_variable([1, 1, 225, 128])\n",
    "    b_conv7 = bias_variable([128])\n",
    "    h_conv7 = tf.nn.conv2d(h_pool2, W_conv7, strides=[1, 1, 1, 1], padding='SAME') + b_conv7\n",
    "    h_conv7_BN = tf.layers.batch_normalization(h_conv7, training=True)\n",
    "    h_conv7_Acti = tf.nn.leaky_relu(h_conv7_BN)\n",
    "    h_conv7_drop = tf.nn.dropout(h_conv7_Acti, keep_prob, noise_shape=[tf.shape(h_conv7_Acti)[0], 1, 1, tf.shape(h_conv7_Acti)[3]])\n",
    "\n",
    "    # Eighth Convolutional Layer\n",
    "    W_conv8 = weight_variable([3, 3, 128, 128])\n",
    "    b_conv8 = bias_variable([128])\n",
    "    h_conv8 = tf.nn.conv2d(h_conv7_drop, W_conv8, strides=[1, 1, 1, 1], padding='SAME') + b_conv8\n",
    "    h_conv8_BN = tf.layers.batch_normalization(h_conv8, training=True)\n",
    "    h_conv8_Acti = tf.nn.leaky_relu(h_conv8_BN)\n",
    "    h_conv8_drop = tf.nn.dropout(h_conv8_Acti, keep_prob, noise_shape=[tf.shape(h_conv8_Acti)[0], 1, 1, tf.shape(h_conv8_Acti)[3]])\n",
    "\n",
    "    # Ninth Convolutional Layer\n",
    "    W_conv9 = weight_variable([1, 1, 128, 256])\n",
    "    b_conv9 = bias_variable([256])\n",
    "    h_conv9 = tf.nn.conv2d(h_conv8_drop, W_conv9, strides=[1, 1, 1, 1], padding='SAME') + b_conv9\n",
    "    h_conv9_BN = tf.layers.batch_normalization(h_conv9, training=True)\n",
    "    h_conv9_res = tf.concat([h_conv9_BN, h_pool2], axis=3)  # 481 feature maps now == 225 + 256\n",
    "    h_conv9_Acti = tf.nn.leaky_relu(h_conv9_res)\n",
    "    h_conv9_drop = tf.nn.dropout(h_conv9_Acti, keep_prob, noise_shape=[tf.shape(h_conv9_Acti)[0], 1, 1, tf.shape(h_conv9_Acti)[3]])\n",
    "\n",
    "    # Third Max Pooling Layer\n",
    "    h_pool3 = tf.nn.max_pool(h_conv9_drop, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Flatten Layer\n",
    "    h_pool6_flat = tf.reshape(h_pool3, [-1, 8 * 8 * 481])\n",
    "\n",
    "    # First Fully Connected Layer\n",
    "    W_fc1 = weight_variable([8 * 8 * 481, 512])\n",
    "    b_fc1 = bias_variable([512])\n",
    "    h_fc1 = tf.matmul(h_pool6_flat, W_fc1) + b_fc1\n",
    "    h_fc1_BN = tf.layers.batch_normalization(h_fc1, training=True)\n",
    "    h_fc1_Acti = tf.nn.leaky_relu(h_fc1_BN)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1_Acti, keep_prob)\n",
    "\n",
    "    # Second Fully Connected Layer\n",
    "    W_fc2 = weight_variable([512, 4])\n",
    "    b_fc2 = bias_variable([4])\n",
    "    prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_chans = X_train.shape[1]\n",
    "X_train = tf.placeholder(tf.float32, [None, 4096])\n",
    "y_train = tf.placeholder(tf.float32, [None, 4])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "model = CNN(Input=X_train, keep_prob=keep_prob)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor = 0.5, patience = 5, min_lr = 0.00001, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, (70, 22, 750, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "\n",
    "total_epoch = -1\n",
    "Tlosses, Taccuracies = [], []\n",
    "Vlosses, Vaccuracies = [], []\n",
    "highest_acc = 0\n",
    "\n",
    "savename = \"Model/cnn_model_mi1.pth\"\n",
    "\n",
    "start=time.time()\n",
    "for i_epoch in range(100):\n",
    "    total_epoch += 1\n",
    "    # Randomize batches ids and get iterater 'i_trials_in_batch'\n",
    "    i_trials_in_batch = get_balanced_batches(len(train_set.X), rng, shuffle=True,\n",
    "                                             batch_size=batch_size)\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    for i_trials in i_trials_in_batch:\n",
    "        # Have to add empty fourth dimension to X for training\n",
    "        batch_X = train_set.X[i_trials][:, :, :, None]\n",
    "        batch_y = train_set.y[i_trials]\n",
    "        # convert from nparray to torch tensor\n",
    "        net_in = np_to_var(batch_X)\n",
    "        if cuda:\n",
    "            net_in = net_in.cuda()\n",
    "        net_target = np_to_var(batch_y)\n",
    "        if cuda:\n",
    "            net_target = net_target.cuda()\n",
    "        # Remove gradients of last backward pass from all parameters\n",
    "        optimizer.zero_grad()\n",
    "        # Compute outputs of the network\n",
    "        outputs = model(net_in)\n",
    "        # Compute the loss\n",
    "        loss = F.nll_loss(outputs, net_target)\n",
    "        # Do the backpropagation\n",
    "        loss.backward()\n",
    "        # Update parameters with the optimizer\n",
    "        optimizer.step()\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    print(\"Epoch {:d}\".format(total_epoch))\n",
    "    average_acc = []\n",
    "    average_loss = []\n",
    "    \n",
    "    # Here we compute training accuracy and validation accuracy of current model\n",
    "    for setname, dataset in (('Train', train_set), ('Valid', valid_set)):\n",
    "        i_trials_in_batch = get_balanced_batches(len(dataset.X), rng, shuffle=False,\n",
    "                                                 batch_size=batch_size)\n",
    "        outputs=None\n",
    "        for i_trials in i_trials_in_batch:\n",
    "            batch_X = dataset.X[i_trials][:, :, :, None]\n",
    "            batch_y = dataset.y[i_trials]\n",
    "            net_in = np_to_var(batch_X)\n",
    "            if cuda:\n",
    "                net_in = net_in.cuda()\n",
    "            toutputs = model(net_in)\n",
    "            if outputs is None:\n",
    "                temp = toutputs.cpu()\n",
    "                outputs = temp.detach().numpy()\n",
    "            else:\n",
    "                temp = toutputs.cpu()\n",
    "                outputs = np.concatenate((outputs,temp.detach().numpy()))\n",
    "        net_target = np_to_var(dataset.y)\n",
    "        loss = F.nll_loss(torch.from_numpy(outputs), net_target)\n",
    "        print(\"{:6s} Loss: {:.5f}\".format(\n",
    "            setname, float(var_to_np(loss))))\n",
    "        predicted_labels = np.argmax((outputs), axis=1)\n",
    "        accuracy = np.mean(dataset.y  == predicted_labels)\n",
    "        \n",
    "        print(\"{:6s} Accuracy: {:.1f}%\".format(setname, accuracy * 100))\n",
    "        if setname == 'Train':\n",
    "            Tlosses.append(loss)\n",
    "            Taccuracies.append(accuracy)\n",
    "            current_Tacc=accuracy\n",
    "        elif setname == 'Valid':\n",
    "            Vlosses.append(loss)\n",
    "            Vaccuracies.append(accuracy)\n",
    "            scheduler.step(loss)\n",
    "            if accuracy>=highest_acc:\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict()\n",
    "                }, savename)\n",
    "\n",
    "                highest_acc=accuracy\n",
    "                print('model saved')\n",
    "                plot_confusion_matrix(dataset.y, predicted_labels, \n",
    "                                      classes=['LH', 'RH', 'Other'], normalize=True,\n",
    "                                      title='Validation confusion matrix')\n",
    "                plt.show()\n",
    "        else:\n",
    "            average_acc.append(accuracy)\n",
    "            average_loss.append(accuracy)\n",
    "end = time.time()\n",
    "\n",
    "print('time is {} min'.format(int((end-start)/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shallow_net-> 100 epoches, batch_size = 100, train_acc = 58.3% and valid_acc = 61.8% and test_acc = 59.3%\n",
    "\n",
    "Motor_net2-> 100 epoches, batch_size = 100, train_acc = 74.1% and valid_acc = 66.1% and test_acc = 64.1%\n",
    "\n",
    "Motor_net3-> 100 epoches, batch_size = 50, train_acc = 66.7% and valid_acc = 64.7% and test_acc =62.6 %\n",
    "\n",
    "Motor_net3_new-> 100 epoches, batch_size = 30, train_acc = 72.7% and valid_acc = 66.1% and test_acc =66.6 %\n",
    "\n",
    "Motor_net4-> 78 epoches, batch_size = 20, train_acc = 68.0% and valid_acc = 57.0% and test_acc =55.1 %\n",
    "\n",
    "Motor_net7-> 100 epoches, batch_size = 30, train_acc = 49.8% and valid_acc = 51.2% and test_acc = 46.1 %\n",
    "\n",
    "Motor_net8-> 100 epoches, batch_size = 30, train_acc = 78.8 % and valid_acc = 67.5 % and test_acc = 69.8 %\n",
    "\n",
    "Motor_net9-> 100 epoches, batch_size = 30, train_acc = 81.5 % and valid_acc = 68.2 % and test_acc = 68.9 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0.0, len(Tlosses), 1)+1\n",
    "plt.plot(t, Tlosses, 'r', t, Vlosses, 'y')\n",
    "plt.legend(('Training loss', 'validation loss'))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(t, Taccuracies, 'r', t, Vaccuracies, 'y')\n",
    "plt.legend(('Training accuracy', 'Validation accuracy'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DpEEG_net()\n",
    "savename = \"Model/cnn_model_mi.pth\"\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "checkpoint = torch.load(savename)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "average_acc, average_loss = [], []\n",
    "setname = 'testset'\n",
    "dataset = test_set\n",
    "\n",
    "i_trials_in_batch = get_balanced_batches(len(dataset.X), rng, shuffle=False,\n",
    "                                         batch_size=30)\n",
    "outputs=None\n",
    "for i_trials in i_trials_in_batch:\n",
    "    # Have to add empty fourth dimension to X\n",
    "    batch_X = dataset.X[i_trials][:, :, :, None]\n",
    "    batch_y = dataset.y[i_trials]\n",
    "    net_in = np_to_var(batch_X)\n",
    "    if cuda:\n",
    "        net_in = net_in.cuda()\n",
    "    toutputs = model(net_in)\n",
    "    if outputs is None:\n",
    "        temp = toutputs.cpu()\n",
    "        outputs = temp.detach().numpy()\n",
    "    else:\n",
    "        temp = toutputs.cpu()\n",
    "        outputs = np.concatenate((outputs,temp.detach().numpy()))\n",
    "\n",
    "net_target = np_to_var(dataset.y)\n",
    "loss = F.nll_loss(torch.from_numpy(outputs), net_target)\n",
    "print(\"{:6s} Loss: {:.5f}\".format(setname, float(var_to_np(loss))))\n",
    "predicted_labels = np.argmax((outputs), axis=1)\n",
    "accuracy = np.mean(dataset.y  == predicted_labels)\n",
    "\n",
    "print(\"{:6s} Accuracy: {:.1f}%\".format(setname, accuracy * 100))\n",
    "plot_confusion_matrix(dataset.y, predicted_labels, \n",
    "                      classes=['LH','RH','Other'], normalize=True,\n",
    "                      title='Validation confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
